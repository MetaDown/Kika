{"nbformat":4,"nbformat_minor":2,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.7.11 64-bit ('py_tf': conda)"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"},"colab":{"name":"UNET-api.ipynb","provenance":[]},"interpreter":{"hash":"ec549830669b736237f7d5bc49e5807896bc58d84b6f7c122708164714f93445"}},"cells":[{"cell_type":"code","execution_count":1,"source":["from tensorflow.keras.layers import Conv2D, BatchNormalization, Conv2DTranspose, MaxPooling2D, UpSampling2D, concatenate, Dropout\n","from tensorflow.keras.callbacks import EarlyStopping, History, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n","from tensorflow.keras.preprocessing.image import load_img\n","import matplotlib.pyplot as plt\n","from datetime import datetime\n","import tensorflow as tf\n","import numpy as np\n","import random\n","import cv2\n","import os"],"outputs":[],"metadata":{"id":"kLDzqhfUZd4Z"}},{"cell_type":"code","execution_count":2,"source":["gpus = tf.config.list_physical_devices('GPU')\n","print(\"Los dispositivos encontrados son: \", gpus)\n","if gpus:\n","  try:\n","    for gpu in gpus:\n","      tf.config.experimental.set_memory_growth(gpu, True)\n","    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n","    print(len(gpus), \"GPUs físicos,\", len(logical_gpus), \"GPUs lógicos\")\n","  except RuntimeError as e:\n","    print(e)\n","print(\"----------------------------------------------------- \\n\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Los dispositivos encontrados son:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n","1 GPUs físicos, 1 GPUs lógicos\n","----------------------------------------------------- \n","\n"]}],"metadata":{"id":"XVYzmFgeZd4c","outputId":"930cefa5-7096-42ff-f884-b70eba3f9a6e"}},{"cell_type":"code","execution_count":5,"source":["#Dirección de directorios de entrenamient y validación\n","input_dir= \"/home/revientaelp/Documentos/Bases de datos/Kika/300/ent/org\"\n","target_dir= \"/home/revientaelp/Documentos/Bases de datos/Kika/300/ent/msk\"\n","\n","val_input_it= \"/home/revientaelp/Documentos/Bases de datos/Kika/300/val/org\"\n","val_target_it= \"/home/revientaelp/Documentos/Bases de datos/Kika/300/val/msk\"\n","\n","#Listas de las imagenes\n","img_size = (304, 304)\n","num_classes = 2\n","batch_size_t = 1\n","batch_size_v = 1\n","\n","input_img_paths = sorted(\n","    [\n","        os.path.join(input_dir, fname)\n","        for fname in os.listdir(input_dir) \n","        if fname.endswith(\".png\")\n","    ]\n",")\n","\n","target_img_paths = sorted(\n","    [\n","        os.path.join(target_dir, fname)\n","        for fname in os.listdir(target_dir)\n","        if fname.endswith(\".png\") and not fname.startswith(\".\")\n","    ]\n",")\n","\n","val_input_paths = sorted(\n","    [\n","        os.path.join(val_input_it, fname)\n","        for fname in os.listdir(val_input_it)\n","        if fname.endswith(\".png\") and not fname.startswith(\".\")\n","    ]\n",")\n","\n","val_target_paths = sorted(\n","    [\n","        os.path.join(val_target_it, fname)\n","        for fname in os.listdir(val_target_it)\n","        if fname.endswith(\".png\") and not fname.startswith(\".\")\n","    ]\n",")\n","\n","ID_input_t= np.arange(len(input_img_paths))\n","ID_input_v= np.arange(len(val_input_paths))"],"outputs":[],"metadata":{"id":"sRvjjlCaZd4d"}},{"cell_type":"code","execution_count":6,"source":["#Generador de los ejemplos de entrada\n","class MiClasificacion(tf.keras.utils.Sequence):\n","    def __init__(self, input_img_paths, target_img_paths, ID_input, shuffle= True, batch_size= 32, img_size= (304, 304, 3), train= True):\n","\n","        self.batch_size = batch_size\n","        self.img_size = img_size\n","        self.input_img_paths = input_img_paths\n","        self.target_img_paths = target_img_paths\n","        self.ID_input = ID_input\n","        self.shuffle= shuffle\n","        self.cont= 0\n","        self.train= train\n","\n","    def __len__(self):\n","        # Calcula el numero de pasos por epoca.\n","        return len(self.target_img_paths) // self.batch_size\n","\n","    def __getitem__(self, idx):\n","        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n","\n","        # Combinamos los datos en cada epoca, pero solo para los datos de entrenamiento\n","        if (self.cont== len(self.target_img_paths) // self.batch_size) or (self.cont== 0) and (self.train== True):\n","            np.random.shuffle(self.ID_input)\n","            self.cont= 0\n","            \n","        i = idx * self.batch_size\n","        batch_img= []\n","        batch_tar= []\n","        if self.train== True:\n","            self.cont =self.cont +1\n","\n","        #De acuerdo a los indices en ID_input tomamos las imagenes de los paths correspondientes\n","        for ig in self.ID_input[i : i + self.batch_size]:\n","            batch_img.append(self.input_img_paths[ig])\n","            batch_tar.append(self.target_img_paths[ig])\n","\n","        X, Y= self.__data_generation(batch_img, batch_tar)                                          # Mandamos llamar a la funcion generadora de los ejemplos\n","\n","        return X, Y\n","\n","    def __data_generation(self, biip, btip):\n","            \n","        # Creacion del tensor con dimensiones las dimensiones de entrada\n","        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n","        #x = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"float32\")                    # Generamos un tensor que almacenara las imagenes importadas     \n","        for j, path in enumerate(biip):\n","            img = load_img(path, target_size=self.img_size)                                         # Cragamos las imagenes de entrada \n","            x[j] = np.array(img)/255.0                                                              # Normalizamos y agregamos la imagen al tensor\n","\n","        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")                      # Generamos un tensor que almacenara las mascaras importadas \n","        for j, path in enumerate(btip):\n","            img = load_img(path,target_size= self.img_size,color_mode=\"grayscale\")                  # Importamos la imagne en escala de grises\n","            y[j] = np.expand_dims(img, 2)\n","            for r in range(y.shape[1]):\n","                for g in range(y.shape[2]):\n","                    if y[j, r, g, 0]!= 0:                                                           # Nos aseguramos de que las venas etiquetadas resulten se un 1\n","                        y[j, r, g, 0]= 1\n","\n","        return x, y"],"outputs":[],"metadata":{"id":"ZaGKznb6Zd4e"}},{"cell_type":"code","execution_count":7,"source":["# Generamos la secuencia del DownSampling de UNET\n","def conv_code(entrada, filtros, ker_reg= None):\n","  x= Conv2D(filtros, 3, padding= \"same\", activation= \"relu\",  kernel_regularizer= ker_reg)(entrada)\n","  x= BatchNormalization()(x)\n","\n","  x= Conv2D(filtros, 3, padding= \"same\", activation= \"relu\", kernel_regularizer= ker_reg)(x)\n","  x= BatchNormalization()(x)\n","\n","  residual= x\n","  x= MaxPooling2D(3, strides= 2, padding= \"same\")(x)\n","  \n","  return residual, x\n","\n","# Generamos la secuencia BottleNeck correspondiente a l\n","def botle(entrada):\n","  x= Conv2D(512, 3, padding= \"same\", activation= \"relu\",  kernel_regularizer= tf.keras.regularizers.L1L2(l1=0.001, l2=0.001))(entrada)\n","  x= BatchNormalization()(x)\n","  x= Conv2D(512, 3, padding= \"same\", activation= \"relu\",  kernel_regularizer= tf.keras.regularizers.L1L2(l1=0.001, l2=0.001))(x)\n","  x= BatchNormalization()(x)\n","\n","  x= Conv2DTranspose(256, 3,padding= \"same\" )(x)\n","\n","  return x\n","\n","# Generamos el UPSAMPLING \n","def conv_decode(entrada, filtros, ker_reg= None):\n","  x= Conv2D(filtros, 3, padding= \"same\", activation= \"relu\", kernel_regularizer= ker_reg)(entrada)\n","  x= BatchNormalization()(x)\n","  x= Conv2D(filtros, 3, padding= \"same\", activation= \"relu\", kernel_regularizer= ker_reg)(x)\n","  x= BatchNormalization()(x)\n","\n","  x= Conv2DTranspose(int(filtros/2), 3,padding= \"same\" )(x)\n","\n","  return x\n","\n","# Para ir subiendo entre capa y capa\n","def crop_and_cat(entrada, res):\n","  x= UpSampling2D(2)(entrada)\n","  \n","  return concatenate([x, res])\n","\n","#La capa final de salida\n","def capa_final(entrada, filtros, num_clases):\n","  x= Conv2D(filtros, 3, padding= \"same\", activation= \"relu\")(entrada)\n","  x= BatchNormalization()(x)\n","\n","  x= Conv2D(filtros, 3, padding= \"same\", activation= \"relu\")(x)\n","  x= BatchNormalization()(x)\n","\n","  x= Conv2D(num_clases, 3, padding= \"same\", activation= \"softmax\")(x)\n","\n","  return x"],"outputs":[],"metadata":{"id":"r7kPeHaoZd4f"}},{"cell_type":"code","execution_count":8,"source":["#Generamos el modelo\n","def get_model(img_size, num_classes):\n","  inputs= tf.keras.Input(shape= img_size+ (3,))\n","  \n","  res_1, conv_1= conv_code(inputs, 64)\n","  res_2, conv_2= conv_code(conv_1, 128)\n","  res_3, conv_3= conv_code(conv_2, 256, tf.keras.regularizers.L1L2(l1=0.001, l2=0.001))\n","\n","  bot= botle(conv_3)\n","\n","  crop1= crop_and_cat(bot, res_3)\n","  decode_3= conv_decode(crop1, 256, tf.keras.regularizers.L1L2(l1=0.001, l2=0.001))\n","\n","  crop2= crop_and_cat(decode_3, res_2)\n","  decode_2= conv_decode(crop2, 128)\n","\n","  crop3= crop_and_cat(decode_2, res_1)\n","  x= Dropout(rate= 0.5)(crop3)\n","  outputs= capa_final(x, 64, 2)\n","\n","  model= tf.keras.Model(inputs, outputs)\n","  return model\n","\n","def entrenamient_modelo(train_gen, val_gen, DIR_save, epocas= 30, visualizacion= False):\n","\n","    #Generamos los callback necesarios\n","    checkpoint= ModelCheckpoint(DIR_save+ \"02092021_S1.hdf5\", monitor= 'val_loss', save_best_only= True, mode= 'min', save_weights_only= False)\n","    reduceLROnPlat= ReduceLROnPlateau(monitor= 'val_loss', factor= 0.8, patience= 3, min_delta= 0.001, cooldown= 7, min_lr= 0.0001)\n","    #tensorboard_callback = TensorBoard(log_dir= datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","    callbacks= [EarlyStopping(patience= 12,  monitor='val_loss'), History(), checkpoint, reduceLROnPlat]#, tensorboard_callback]\n","    metrica= [tf.keras.metrics.CategoricalAccuracy()]\n","\n","    #Compilamos con la funcion perdida y el optimizador que utilizaremos\n","    model.compile(optimizer=\"SGD\", loss= \"sparse_categorical_crossentropy\", metrics= metrica) #loss= \"binary_crossentropy\", metrics= ['accuracy'])\n","    history= model.fit(train_gen, epochs=epocas, validation_data=val_gen, callbacks=callbacks)\n","  \n","def infor_model(img_size):\n","  model = get_model(img_size, 2)                                   # Generamos el modelo con el tamaño de las imagenes de entrada y la salida esperada\n","  model.summary()  \n","\n","def carga_mod(DIR):\n","    return tf.keras.models.load_model(DIR)"],"outputs":[],"metadata":{"id":"tU2CwEL1Zd4f"}},{"cell_type":"code","execution_count":9,"source":["#Creamos los generadoras para el entrenamiento y la validación\n","train_gen = MiClasificacion(\n","    input_img_paths, target_img_paths, ID_input_t, shuffle= True, batch_size= batch_size_t, img_size= img_size, train= True\n",")\n","\n","val_gen = MiClasificacion(\n","    val_input_paths, val_target_paths, ID_input_v, shuffle= True, batch_size=batch_size_v, img_size= img_size, train= False\n",")\n","\n","infor_model((304, 304))"],"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 304, 304, 3) 0                                            \n","__________________________________________________________________________________________________\n","conv2d (Conv2D)                 (None, 304, 304, 64) 1792        input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 304, 304, 64) 256         conv2d[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 304, 304, 64) 36928       batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 304, 304, 64) 256         conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d (MaxPooling2D)    (None, 152, 152, 64) 0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 152, 152, 128 73856       max_pooling2d[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 152, 152, 128 512         conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 152, 152, 128 147584      batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 152, 152, 128 512         conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 76, 76, 128)  0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 76, 76, 256)  295168      max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 76, 76, 256)  1024        conv2d_4[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 76, 76, 256)  590080      batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 76, 76, 256)  1024        conv2d_5[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 38, 38, 256)  0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 38, 38, 512)  1180160     max_pooling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 38, 38, 512)  2048        conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 38, 38, 512)  2359808     batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 38, 38, 512)  2048        conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_transpose (Conv2DTranspo (None, 38, 38, 256)  1179904     batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","up_sampling2d (UpSampling2D)    (None, 76, 76, 256)  0           conv2d_transpose[0][0]           \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 76, 76, 512)  0           up_sampling2d[0][0]              \n","                                                                 batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 76, 76, 256)  1179904     concatenate[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 76, 76, 256)  1024        conv2d_8[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 76, 76, 256)  590080      batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 76, 76, 256)  1024        conv2d_9[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_transpose_1 (Conv2DTrans (None, 76, 76, 128)  295040      batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","up_sampling2d_1 (UpSampling2D)  (None, 152, 152, 128 0           conv2d_transpose_1[0][0]         \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 152, 152, 256 0           up_sampling2d_1[0][0]            \n","                                                                 batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 152, 152, 128 295040      concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 152, 152, 128 512         conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 152, 152, 128 147584      batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 152, 152, 128 512         conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_transpose_2 (Conv2DTrans (None, 152, 152, 64) 73792       batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","up_sampling2d_2 (UpSampling2D)  (None, 304, 304, 64) 0           conv2d_transpose_2[0][0]         \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 304, 304, 128 0           up_sampling2d_2[0][0]            \n","                                                                 batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 304, 304, 128 0           concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 304, 304, 64) 73792       dropout[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 304, 304, 64) 256         conv2d_12[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 304, 304, 64) 36928       batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 304, 304, 64) 256         conv2d_13[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 304, 304, 2)  1154        batch_normalization_13[0][0]     \n","==================================================================================================\n","Total params: 8,569,858\n","Trainable params: 8,564,226\n","Non-trainable params: 5,632\n","__________________________________________________________________________________________________\n"]}],"metadata":{"id":"8CSJHp7jZd4g"}},{"cell_type":"code","execution_count":10,"source":["# Comenzamos el entrenamiento\n","#DIR= 'E:/rortiz324/Proyecto/Bases_de_datos/ModelosGuardados/Grises/SGD/140521/14052021_400.hdf5'\n","#DIR= 'E:/rortiz324/Proyecto/Bases_de_datos/ModelosGuardados/Filtros/SGD/120521/12052021_400.hdf5'\n","#DIR= 'E:/rortiz324/Proyecto/Bases_de_datos/ModelosGuardados/Normales/SGD/140521/14052021_400.hdf5'\n","#DIR= 'E:/rortiz324/Proyecto/Bases_de_datos/ModelosGuardados/Comb/SGD/150521/15052021_400.hdf5'\n","\n","#DIR= 'E:/rortiz324/Proyecto/Bases_de_datos/ModelosGuardados/Filtros/ADAM/240421/24042021_400_2.hdf5'\n","#DIR= 'E:/rortiz324/Proyecto/Bases_de_datos/ModelosGuardados/Normales/ADAM/040521_1/04052021_400.hdf5'\n","#DIR= 'E:/rortiz324/Proyecto/Bases_de_datos/ModelosGuardados/Comb/ADAM/080521/08052021_400.hdf5'\n","#DIR= 'E:/rortiz324/Proyecto/Bases_de_datos/ModelosGuardados/Grises/ADAM/300421/29042021_400_1.hdf5'\n","\n","#model= carga_mod(DIR)\n","model= get_model(img_size, 2)\n","\n","DIR_save_best= '/home/revientaelp/Documentos/Modelos/'\n","entrenamient_modelo(train_gen, val_gen, DIR_save_best, 400, False)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/400\n","7133/7133 [==============================] - 844s 118ms/step - loss: 13.2360 - categorical_accuracy: 0.8992 - val_loss: 0.4080 - val_categorical_accuracy: 0.8951 - lr: 0.0100\n","Epoch 2/400\n","7133/7133 [==============================] - 823s 115ms/step - loss: 0.2168 - categorical_accuracy: 0.8945 - val_loss: 0.3451 - val_categorical_accuracy: 0.9209 - lr: 0.0100\n","Epoch 3/400\n","7133/7133 [==============================] - 822s 115ms/step - loss: 0.2060 - categorical_accuracy: 0.8917 - val_loss: 0.3576 - val_categorical_accuracy: 0.9035 - lr: 0.0100\n","Epoch 4/400\n","7133/7133 [==============================] - 824s 116ms/step - loss: 0.1990 - categorical_accuracy: 0.8901 - val_loss: 0.4144 - val_categorical_accuracy: 0.9233 - lr: 0.0100\n","Epoch 5/400\n","7133/7133 [==============================] - 824s 116ms/step - loss: 0.1945 - categorical_accuracy: 0.8889 - val_loss: 0.3185 - val_categorical_accuracy: 0.9273 - lr: 0.0100\n","Epoch 6/400\n","7133/7133 [==============================] - 824s 116ms/step - loss: 0.1906 - categorical_accuracy: 0.8881 - val_loss: 0.3328 - val_categorical_accuracy: 0.9092 - lr: 0.0100\n","Epoch 7/400\n","7133/7133 [==============================] - 826s 116ms/step - loss: 0.1875 - categorical_accuracy: 0.8875 - val_loss: 0.3146 - val_categorical_accuracy: 0.9073 - lr: 0.0100\n","Epoch 8/400\n","7133/7133 [==============================] - 825s 116ms/step - loss: 0.1848 - categorical_accuracy: 0.8870 - val_loss: 0.3415 - val_categorical_accuracy: 0.9306 - lr: 0.0100\n","Epoch 9/400\n","4919/7133 [===================>..........] - ETA: 3:50 - loss: 0.1832 - categorical_accuracy: 0.8864"]}],"metadata":{"id":"vq6s4-j2Zd4h"}},{"cell_type":"code","execution_count":null,"source":["import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import numpy as np\n","\n","#from IPython.display import Image, display\n","from tensorflow.keras.preprocessing.image import load_img\n","import PIL\n","from PIL import ImageOps\n","m = tf.keras.metrics.AUC()\n","\n","\n","val_input_it= \"E:/rortiz324/Proyecto/Bases_de_datos/Imagenes_Grises/Validacion/Img\"\n","val_target_it= \"E:/rortiz324/Proyecto/Bases_de_datos/Imagenes_Grises/Validacion/Mask\"\n","\n","#val_input_it= \"E:/rortiz324/Proyecto/Bases_de_datos/Imagenes_Mejoradas/Validacion_Total/Img\"\n","#val_target_it= \"E:/rortiz324/Proyecto/Bases_de_datos/Imagenes_Mejoradas/Validacion_Total/Mask\"\n","\n","#val_input_it= \"E:/rortiz324/Proyecto/Bases_de_datos/Imagenes_Normales/Validacion_Total/Img\"\n","#val_target_it= \"E:/rortiz324/Proyecto/Bases_de_datos/Imagenes_Normales/Validacion_Total/Mask\"\n","\n","#val_input_it= \"E:/rortiz324/Proyecto/Bases_de_datos/Imagenes_Combinadas/Validacion_T/Img\"\n","#val_target_it= \"E:/rortiz324/Proyecto/Bases_de_datos/Imagenes_Combinadas/Validacion_T/Mask\"\n","\n","ret= 0\n","\n","val_input_paths = sorted(\n","    [\n","        os.path.join(val_input_it, fname)\n","        for fname in os.listdir(val_input_it)\n","        if fname.endswith(\".png\") and not fname.startswith(\".\")\n","    ]\n",")\n","\n","val_target_paths = sorted(\n","    [\n","        os.path.join(val_target_it, fname)\n","        for fname in os.listdir(val_target_it)\n","        if fname.endswith(\".png\") and not fname.startswith(\".\")\n","    ]\n",")\n","\n","\n","for l in range(10):\n","    img_1= load_img(val_input_paths[l], target_size=(400, 400, 3))\n","    img_1= (np.array(img_1).reshape(-1,400,400,3))/255.0\n","    val_preds = model.predict(img_1)\n","    mask = np.argmax(val_preds[0], axis=-1)\n","    mask = np.expand_dims(mask, axis=-1)\n","\n","    im = load_img(val_target_paths[l], target_size= (400, 400), color_mode=\"grayscale\")  \n","    y= np.array(im).reshape(400,400,1)\n","    for r in range(y.shape[0]):\n","        for g in range(y.shape[1]):\n","            if y[r, g, 0]!= 0:\n","                y[r, g, 0]= 1\n","\n","    m.update_state(y, mask)\n","    ret= ret + m.result().numpy()\n","\n","ret= ret/10\n","print(ret)\n","    \n","\n","#img = PIL.ImageOps.autocontrast(tf.keras.preprocessing.image.array_to_img(mask))\n","#plt.imshow(img, cmap=\"gray\")\n","#plt.show()\n","#plt.imsave(\"SGD_Norm.png\", img, cmap=\"gray\")\n","#display(img)"],"outputs":[],"metadata":{"id":"G0OivsaYZd4i","outputId":"a02d18f3-c40f-4d87-e68a-b2b3cf2dc239"}},{"cell_type":"code","execution_count":null,"source":["import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import numpy as np\n","\n","#from IPython.display import Image, display\n","from tensorflow.keras.preprocessing.image import load_img\n","import PIL\n","from PIL import ImageOps\n","\n","val_input_it= \"E:/rortiz324/Proyecto/Bases_de_datos/Imagenes_Grises/Validacion/Img/0.png\"\n","\n","img_1= load_img(val_input_it, target_size=(400, 400, 3))\n","img_1= (np.array(img_1).reshape(-1,400,400,3))/255.0\n","val_preds = model.predict(img_1)\n","mask = np.argmax(val_preds[0], axis=-1)\n","mask = np.expand_dims(mask, axis=-1)\n","\n","img = PIL.ImageOps.autocontrast(tf.keras.preprocessing.image.array_to_img(mask))\n","plt.imshow(img, cmap=\"gray\")\n","plt.show()\n","plt.imsave(\"SGD_Comb.png\", img, cmap=\"gray\")"],"outputs":[],"metadata":{"id":"jV5KuSElZd4j","outputId":"6cccdda0-6721-463b-89d3-a2e1a0cad875"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"gt5jjblsZd4j"}}]}